{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Description: You are given a dataset which contains satellite images from Texas after Hurricane Harvey. There are damaged and non-damaged building images organized into respective folders. You can find the project 3 dataset on the course GitHub repository here.\n",
    "\n",
    "Your goal is to build multiple neural networks based on different architectures to classify images as containing buildings that are either damaged or not damaged. You will evaluate each of the networks you develop and produce and select the “best” network to “deploy”. Note that this is a binary classification problem, where the goal it to classify whether the structure in the image has damage or does not have damage.\n",
    "\n",
    "Part 1: (3 points) Data preprocessing and visualization\n",
    "\n",
    "You will need to perform data analysis and pre-processing to prepare the images for training. At a minimum, you should:\n",
    "\n",
    "Write code to load the data into Python data structures\n",
    "\n",
    "Investigate the datasets to determine basic attributes of the images\n",
    "\n",
    "Ensure data is split for training, validation and testing and perform any additional preprocessing (e.g., rescaling, normalization, etc.) so that it can be used for training/evaluation of the neural networks you will build in Part 2.\n",
    "\n",
    "Part 2: (10 points) Model design, training and evaluation\n",
    "\n",
    "You will explore different model architectures that we have seen in class, including:\n",
    "\n",
    "A dense (i.e., fully connected) ANN\n",
    "\n",
    "The Lenet-5 CNN architecture\n",
    "\n",
    "Alternate-Lenet-5 CNN architecture, described in paper/except (Table 1, Page 12 of the research paper https://arxiv.org/pdf/1807.01688.pdf, but note that the dataset is not the same as that analyzed in the paper.)\n",
    "\n",
    "You are free to experiment with different variants on all three architectures above. For example, for the fully connected ANN, feel free to experiment with different numbers of layers and perceptrons. Train and evaluate each model you build,and select the “best” performing model.\n",
    "\n",
    "Note that the input and output dimensions are fixed, as the inputs (images) and the outputs (labels) have been given. These have important implications for your architecture. Make sure you understand the constraints these impose before beginning to design and implement your networks. Failure to implement these correctly will lead to incorrect architectures and significant penalty on the project grade.\n",
    "\n",
    "Note: You can also try to run the VGG-16 architecture from class, however, you may run into long runtimes and/or memory limits on the VM. It is also possible, depending on the architecture that you choose, that you could also run into memory constraints with any of the other architectures. If you are hitting memory issues, you can try to decrease the batch_size parameter in the .fit() function, as described in the notes.\n",
    "\n",
    "Part 3: (7 points) Model deployment\n",
    "\n",
    "For the best model built in part 2, persist the trained model to disk so that it can be reconstituted easily. Develop a simple inference server to serve your trained model over HTTP. There should be at least two endpoints:\n",
    "\n",
    "A model summary endpoint providing metadata about the model\n",
    "\n",
    "An inference endpoint that can perform classification on an image. Note: this endpoint will need to accept a binary message payload containing the image to classify and return a JSON response containing the results of the inference.\n",
    "\n",
    "Package your model inference server in a Docker container image and push the image to the Docker Hub. Provide instructions for starting and stopping your inference server using the docker-compose file. Provide examples of how to make requests to your inference server.\n",
    "\n",
    "\n",
    "Part 4: (7 points) Write a 3 page report summarizing your work. Be sure to include something about the following:\n",
    "\n",
    "Data preparation: what did you do? (1 pt)\n",
    "\n",
    "Model design: which architectures did you explore and what decisions did you make for each? (2 pts)\n",
    "\n",
    "Model evaluation: what model performed the best? How confident are you in your model? (1 pt)\n",
    "\n",
    "Model deployment and inference: a brief description of how to deploy/serve your model and how to use the model for inference (this material should also be in the README with examples) (1 pt)\n",
    "\n",
    "Submission guidelines: Part 1 and Part 2 should be submitted as one notebook file. Part 3 should include a Dockerfile, a docker image (prebuilt and pushed to Docker Hub) and a docker-compose.yml file for starting the container. It should also include a README with instructions for using the container image, docker-compose file and example requests. Part 4 should be submitted as a PDF file.\n",
    "\n",
    "In-class Project Checkpoint Thursday, April 4th. We will devote the first portion of Thursday’s class to checking in on the project and answering questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "epoch_count = 40 # number of epochs used to train all models\n",
    "base_data_path = \"./coe379L-sp24/datasets/unit03/Project3/data_all_modified/\"\n",
    "damage_file_paths = os.listdir(base_data_path + \"damage/\")\n",
    "no_damage_file_paths = os.listdir(base_data_path + \"no_damage/\")\n",
    "print(\"damage file entry: \", damage_file_paths[0])\n",
    "print(\"No damage file entry: \", no_damage_file_paths[0])\n",
    "print()\n",
    "print(\"Number of damage files: \", len(damage_file_paths))\n",
    "print(\"Number of no_damage files: \", len(no_damage_file_paths))\n",
    "print(\"Total number of files: \", len(damage_file_paths) + len(no_damage_file_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we seek preliminary information about our dataset such as the size of each file, the number of files in each class, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating file paths for each image belonging to both classes\n",
    "damage_file_paths = [base_data_path + \"damage/\" + file for file in os.listdir(base_data_path + \"damage/\")]\n",
    "no_damage_file_paths = [base_data_path + \"no_damage/\" + file for file in os.listdir(base_data_path + \"no_damage/\")]\n",
    "print(f\"damage file entry 1: , '{damage_file_paths[0]}'\")\n",
    "print(f\"No damage file entry 1: , '{no_damage_file_paths[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating file size statistics\n",
    "print(f\"{'='*20} File Size Statistics {'='*20}\")\n",
    "damage_file_sizes = np.array([os.path.getsize(file) for file in damage_file_paths])\n",
    "no_damage_file_sizes = np.array([os.path.getsize(file) for file in no_damage_file_paths])\n",
    "print(f\"Average damage file size : {np.mean(damage_file_sizes)/1000:0.4f}kB\")\n",
    "print(f\"Average no_damage file size: {np.mean(no_damage_file_sizes)/1000:0.4f}kB\")\n",
    "print(f\"Range of damage file sizes : {np.min(damage_file_sizes)/1000:0.4f}kB - {np.max(damage_file_sizes)/1000:0.4f}kB\")\n",
    "print(f\"Range of no_damage file sizes: {np.min(no_damage_file_sizes)/1000:0.4f}kB - {np.max(no_damage_file_sizes)/1000:0.4f}kB\")\n",
    "print(f\"Standard deviation of damage file sizes : {np.std(damage_file_sizes)/1000:0.4f}kB\")\n",
    "print(f\"Standard deviation of no_damage file sizes: {np.std(no_damage_file_sizes)/1000:0.4f}kB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*20} File Dimension Statistics {'='*20}\")\n",
    "from skimage import io\n",
    "# calculating file dimension statistics\n",
    "damage_file_dims = [io.imread(file).shape for file in damage_file_paths]\n",
    "no_damage_file_dims = [io.imread(file).shape for file in no_damage_file_paths]\n",
    "print(f\"Average damage file dimensions : {np.mean(damage_file_dims, axis=0)[:2]}\")\n",
    "print(f\"Average no_damage file dimensions: {np.mean(no_damage_file_dims, axis=0)[:2]}\")\n",
    "print(f\"Range of damage file dimensions : {np.min(damage_file_dims, axis=0)[:2]} - {np.max(damage_file_dims, axis=0)[:2]}\")\n",
    "print(f\"Range of no_damage file dimensions: {np.min(no_damage_file_dims, axis=0)[:2]} - {np.max(no_damage_file_dims, axis=0)[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the statistical measures for both image classes are mostly similar. Furthermore, the dimensions for all the images appear to be 128x128 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets visualize the data to get a better understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# visualizing fist 5 images from damage class\n",
    "print(f'{\"=\"*10} DAMAGE {\"=\"*10}')\n",
    "for i in range(2):\n",
    "    display(Image(filename=damage_file_paths[i]))\n",
    "\n",
    "print()\n",
    "print(f'{\"=\"*10} NO_DAMAGE {\"=\"*10}')\n",
    "# visualizing fist 5 images from no_damage class\n",
    "for i in range(2):\n",
    "    display(Image(filename=no_damage_file_paths[i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Pre-Proccessing\n",
    "Because we will explore both ANN and CNN architectures, we will need to pre-process the images differently. For the ANN, we will flatten the images into a single vector. For the CNN, we will keep the images as 2D arrays using helper functions from libraries such as keras. First we will pre-process the images for our ANN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "# flattening + normalizing all images\n",
    "X_file_paths = damage_file_paths + no_damage_file_paths\n",
    "X = [io.imread(file).flatten()/255.0 for file in X_file_paths]\n",
    "y = np.array([1 for _ in range(len(damage_file_paths))] + [0 for _ in range(len(no_damage_file_paths))])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "X_train, X_test = np.array(X_train), np.array(X_test) # converting to numpy arrays\n",
    "\n",
    "\n",
    "# one-hot encoding the target variable y_train using keras to_categorical method\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_classes = 2\n",
    "y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "print(y_train_categorical[:5])\n",
    "print(\"1st column represents no_damage class and 2nd column represents damage class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have processed the images for our ANN architecture, we can re-format our dataset for our CNN architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Split (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we can prepare the data set and split into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear directories before proceeding\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(\"hurricane/train\")\n",
    "    shutil.rmtree(\"hurricane/test\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two classes: damage and no-damage\n",
    "# create directories for each class within train and test directory\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"hurricane/train/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"hurricane/test/no-damage\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"hurricane/test/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"hurricane/train/no-damage\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating path for each class's data set\n",
    "damage_file_paths = os.listdir(\"./coe379L-sp24/datasets/unit03/Project3/data_all_modified/damage\")\n",
    "no_damage_file_paths = os.listdir(\"./coe379L-sp24/datasets/unit03/Project3/data_all_modified/no_damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split image paths to train and test; 80% in train, 20% in test\n",
    "import random\n",
    "\n",
    "print(f'{\"=\"*10} DAMAGE {\"=\"*10}')\n",
    "train_damage_paths = random.sample(damage_file_paths, int(len(damage_file_paths)*0.8))\n",
    "print(\"Train data set image count: \", len(train_damage_paths))\n",
    "test_damage_paths = [ p for p in damage_file_paths if p not in train_damage_paths]\n",
    "print(\"Test data set image count: \", len(test_damage_paths))\n",
    "\n",
    "overlap = [p for p in train_damage_paths if p in test_damage_paths]\n",
    "print(\"Overlap between train and test sets: \", len(overlap))\n",
    "\n",
    "print(f'\\n{\"=\"*10} NO_DAMAGE {\"=\"*10}')\n",
    "train_no_damage_paths = random.sample(no_damage_file_paths, int(len(no_damage_file_paths)*0.8))\n",
    "print(\"Train data set image count: \", len(train_no_damage_paths))\n",
    "test_no_damage_paths = [ p for p in no_damage_file_paths if p not in train_no_damage_paths]\n",
    "print(\"Test data set image count: \", len(test_damage_paths))\n",
    "\n",
    "overlap = [p for p in train_no_damage_paths if p in test_no_damage_paths]\n",
    "print(\"Overlap between train and test sets: \", len(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy files into respective train and test directories\n",
    "for p in train_damage_paths:\n",
    "    shutil.copyfile(os.path.join(base_data_path + \"damage\", p), os.path.join(\"hurricane/train/damage\", p))\n",
    "\n",
    "for p in test_damage_paths:\n",
    "    shutil.copyfile(os.path.join(base_data_path + \"damage\", p), os.path.join(\"hurricane/test/damage\", p))\n",
    "\n",
    "print(f'{\"=\"*10} DAMAGE {\"=\"*10}')\n",
    "print(\"Files in train/damage: \", len(os.listdir(\"hurricane/train/damage\")))\n",
    "print(\"Files in test/damage: \", len(os.listdir(\"hurricane/test/damage\")))\n",
    "\n",
    "\n",
    "for p in train_no_damage_paths:\n",
    "    print(os.path.join(base_data_path + \"no_damage\", p))\n",
    "    shutil.copyfile(os.path.join(base_data_path + \"no_damage\", p), os.path.join(\"hurricane/train/no-damage\", p))\n",
    "\n",
    "for p in test_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join(base_data_path + \"no_damage\", p), os.path.join(\"hurricane/test/no-damage\", p))\n",
    "\n",
    "print(f'\\n{\"=\"*10} NO_DAMAGE {\"=\"*10}')\n",
    "print(\"Files in train/no-damage: \", len(os.listdir(\"hurricane/train/no-damage\")))\n",
    "print(\"Files in test/no-damage: \", len(os.listdir(\"hurricane/test/no-damage\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing (CNN)\n",
    "Next, we will process the image data for our CNN architectures. We will use the keras library to help us with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: deploy model + write report + write readme\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "\n",
    "image_shape = damage_file_dims[0][:2]\n",
    "\n",
    "# training data processing\n",
    "batch_size = 32\n",
    "train_data_dir = \"hurricane/train\"\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates which dataset is returned\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"both\",\n",
    "color_mode=\"rgb\",\n",
    "seed=123,\n",
    "image_size=image_shape,\n",
    "batch_size=batch_size)\n",
    "\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))\n",
    "\n",
    "# test data processing\n",
    "test_data_dir = \"hurricane/test\"\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates what is returned\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "test_data_dir,\n",
    "seed=123,\n",
    "color_mode=\"rgb\",\n",
    "image_size=image_shape)\n",
    "\n",
    "# approach 1: manually rescale data --\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "test_rescale_ds = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "\n",
    "#### ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper: https://openreview.net/pdf/1WvovwjA7UMnPB1oinBL.pdf\n",
    "# Importing libraries needed for creating neural network\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image_size= np.prod(damage_file_dims[0]) # takes the product of the all dimensions of the image\n",
    "out_put_size = 2\n",
    "num_zlin_pairs = 2\n",
    "# create model. \n",
    "model_ann = Sequential()\n",
    "\n",
    "# input layer (zero biased ReLU)\n",
    "model_ann.add(Dense(784, activation='relu',input_shape=(image_size,), use_bias=False))\n",
    "\n",
    "for i in range(num_zlin_pairs):\n",
    "    # creates a zero-biased-linear pair of layers\n",
    "    model_ann.add(Dense(256, activation='relu', use_bias=False))\n",
    "    model_ann.add(Dense(128, activation='linear', use_bias=True))\n",
    "\n",
    "# final (zero biased ReLU) layer\n",
    "model_ann.add(Dense(256, activation='relu', use_bias=False))\n",
    "\n",
    "# Softmax activation function is selected for multiclass classification\n",
    "model_ann.add(Dense(out_put_size, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_ann.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model_ann.summary()\n",
    "\n",
    "# Fitting\n",
    "model_ann.fit(X_train, y_train_categorical, validation_split=0.2, epochs=epoch_count, batch_size=64, verbose=2)\n",
    "\n",
    "# Saving model\n",
    "model_ann.save('pranjal-coe-379l/project3/models/hurricane_ann_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann.save('pranjal-coe-379l/project3/models/hurricane_ann_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_categorical = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "test_loss, test_accuracy = model_ann.evaluate(X_test, y_test_categorical, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lennet-5 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "\n",
    "model_lenet5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 6 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(no_damage_file_dims[0])))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 16 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_lenet5.add(layers.Flatten())\n",
    "\n",
    "# Layer 3: Fully connected layer with 120 neurons\n",
    "model_lenet5.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_lenet5.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_lenet5.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model_lenet5.summary()\n",
    "\n",
    "# Visualization\n",
    "#TODO: include visualization code here\n",
    "\n",
    "# Fit model\n",
    "model_lenet5.fit(train_rescale_ds, validation_data=val_rescale_ds, epochs=epoch_count, batch_size=64)\n",
    "\n",
    "# saving model\n",
    "model_lenet5.save('pranjal-coe-379l/project3/models/hurricane_lenet5_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_lenet5.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate Lennet-5 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lennet5a = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 32 filters of size 3x3, followed by max pooling\n",
    "model_lennet5a.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(no_damage_file_dims[0])))\n",
    "model_lennet5a.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 64 filters of size 3x3, followed by max pooling\n",
    "model_lennet5a.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_lennet5a.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 3: Convolutional layer with 128 filters of size 3x3, followed by max pooling\n",
    "model_lennet5a.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_lennet5a.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 4: Convolutional layer with 128 filters of size 3x3, followed by max pooling\n",
    "model_lennet5a.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_lennet5a.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_lennet5a.add(layers.Flatten())\n",
    "\n",
    "# Adding dropout layer with a dropout rate of 0.5 to reduce overfitting\n",
    "model_lennet5a.add(layers.Dropout(0.5))\n",
    "\n",
    "# Layer 5: Fully connected layer with 512 neurons\n",
    "model_lennet5a.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model_lennet5a.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lennet5a.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model_lennet5a.summary()\n",
    "\n",
    "# Visualization\n",
    "#TODO: include visualization code here\n",
    "\n",
    "# Fit model\n",
    "model_lennet5a.fit(train_rescale_ds, validation_data=val_rescale_ds, epochs=epoch_count, batch_size=64)\n",
    "\n",
    "# Saving model\n",
    "model_lennet5a.save('pranjal-coe-379l/project3/models/hurricane_lenet5a_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_lennet5a.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: \n",
    "# - run each model (consider adding l2 regularization to alternative cnn model)\n",
    "# - include model visualization code\n",
    "# - look into using data augementation to reduce over-fitting\n",
    "# - look into using the long + lat coordinates in training (somehow)\n",
    "# - complete model evaluation\n",
    "# - model deployment\n",
    "# - write report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
